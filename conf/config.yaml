experiment:
    name: "training_w_pre-trained_encoder" # experiment name shown on wandb and the name of the saved weights
    base_path: "/scratch/cl5503/cost_model_auto_encoder"

data_generation:
    train_dataset_file: "/scratch/cl5503/data/with_buffer_sizes/dataset_expr_batch550000-838143_debug.pkl"  # training / validation set
    valid_dataset_file: "/scratch/cl5503/data/with_buffer_sizes/dataset_expr_batch550000-838143_debug.pkl"
    benchmark_dataset_file: "/data/kb4083/model_release/release_code/result_subsample.json"
    dataset_name:  "original_model_w_itr_log" # dataset for the cost model
    batch_size: 512
    nb_processes: 1
    min_functions_per_tree_footprint: 2

training: 
    log_file: "logs.txt" # Just the name
    lr: 0.001
    max_epochs: 650
    training_gpu: "0" # cuda:0
    validation_gpu: "cpu"
    continue_training: False
    model_weights_path: "/data/kb4083/cost_model/weights/best_model_Baseline_old_patterns_ASPLOS_4675.pt"
    pretrained_weights_path: "/scratch/cl5503/cost_model_auto_encoder/pre_train/saved_models/comps_w_expr_autoencoder_fc_linear_bottleneck.pt"
    use_fraction_data: 1.0 # used to test smaller dataset if set to < 1
    fine_tune_epoch: 100 # the epoch after which the pre-trained weights are unfreezed
    fine_tune_lr: 0.0005 # the learning rate of the pre-trained weights

testing:
    testing_model_weights_path: "/scratch/cl5503/cost_model_auto_encoder/weights/best_model_fc li-bottleneck ae code350 unfreeze_all (refined_sch 2e-4) full data_1778.pt" # Model weights to evaluate
    gpu: "cuda:0" # GPU to validate on

wandb:
    use_wandb: True
    project: PACT24 #"capstone_2024"

model: 
    input_size: 350 # original model: 846, autoencoder25: 574
    comp_embed_layer_sizes:
        - 600
        - 350
        - 200
        - 180
    drops:
        - 0.05
        - 0.05
        - 0.05
        - 0.05

defaults:
  - override hydra/job_logging: disabled